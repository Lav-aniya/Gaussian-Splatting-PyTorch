{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gd2JcPpB6rFO",
    "outputId": "e92de28c-73cc-475a-ecfc-19a41392284e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xeeh50dRNeD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import struct\n",
    "from PIL import Image\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7oHP8hYRkoT"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "json_file = \"/content/drive/MyDrive/dataset/custom-dataset/transforms_1.json\"\n",
    "dataset_root = \"/content/drive/MyDrive/dataset/custom-dataset\"\n",
    "#points3d_path = \"/content/drive/MyDrive/dataset/custom-dataset/colmap_output_1/0/points3D.bin\"\n",
    "points3d_path = \"/content/drive/MyDrive/dataset/custom-dataset/colmap_output_1 /0/points3D.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Pn-ify0THyj"
   },
   "outputs": [],
   "source": [
    "def load_data(json_path, root_dir):\n",
    "  with open(json_path, 'r') as fp:\n",
    "    meta = json.load(fp)\n",
    "\n",
    "  imgs = []\n",
    "  poses = []\n",
    "\n",
    "  # Target size (downscaled for pure PyTorch speed)\n",
    "  W_target = 100\n",
    "  H_target = 178\n",
    "\n",
    "  print(f\"loading images (resizing to {W_target}x{H_target})\")\n",
    "\n",
    "  for frame in tqdm(meta['frames']):\n",
    "    fname = os.path.join(root_dir, frame['file_path'])\n",
    "    if not os.path.exists(fname): continue\n",
    "\n",
    "    img = Image.open(fname).convert('RGB')\n",
    "    img = img.resize((W_target, H_target), Image.LANCZOS)\n",
    "    imgs.append(torch.tensor(np.array(img)).float() / 255.)\n",
    "    poses.append(torch.tensor(frame['transform_matrix']).float())\n",
    "\n",
    "  camera_angle_x = meta['camera_angle_x']\n",
    "  focal = 0.5 * W_target / np.tan(0.5 * camera_angle_x)\n",
    "\n",
    "  return torch.stack(imgs).to(device), torch.stack(poses).to(device), focal, H_target, W_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtCA-xHrTdM0"
   },
   "outputs": [],
   "source": [
    "# BINARY POINT CLOUD LOADER\n",
    "def read_points3D_binary(path_to_model_file):\n",
    "  print(f\"Reading COLMAP point cloud from: {path_to_model_file}\")\n",
    "  points3D = []\n",
    "  colors = []\n",
    "\n",
    "  if not os.path.exists(path_to_model_file):\n",
    "    print(\"points3D.bin not founddd!!!!!!!!!! falling back to random initt\")\n",
    "    return None, None\n",
    "\n",
    "  with open(path_to_model_file, \"rb\") as fid:\n",
    "    num_points = struct.unpack(\"<Q\", fid.read(8))[0]\n",
    "    print(f\"FOUNDD {num_points} points\")\n",
    "\n",
    "    for _ in range(num_points):\n",
    "      binary_point = fid.read(43) # 43 bytes per point\n",
    "      # FORMAT: ID(Q), X(d), Y(d), Z(d), R(B), G(B), B(B), Error(d)\n",
    "      data = struct.unpack(\"<QdddBBBd\", binary_point)\n",
    "\n",
    "      xyz = np.array(data[1:4])\n",
    "      rgb = np.array(data[4:7])\n",
    "\n",
    "      points3D.append(xyz)\n",
    "      colors.append(rgb)\n",
    "\n",
    "      # read track length and skip track data\n",
    "      track_length = struct.unpack(\"<Q\", fid.read(8))[0]\n",
    "      fid.read(track_length * 8)\n",
    "\n",
    "  return np.array(points3D), np.array(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZEy5RpwWxmK"
   },
   "outputs": [],
   "source": [
    "def build_rotation(r):\n",
    "  norm = torch.sqrt(r[:,0]*r[:,0] + r[:,1]*r[:,1] + r[:,2]*r[:,2] + r[:,3]*r[:,3])\n",
    "  q = r / norm[:, None]\n",
    "  R = torch.zeros((q.shape[0], 3, 3), device=device)\n",
    "  r, x, y, z = q[:, 0], q[:, 1], q[:, 2], q[:, 3]\n",
    "\n",
    "  R[:, 0, 0] = 1 - 2 * (y*y + z*z)\n",
    "  R[:, 0, 1] = 2 * (x*y - r*z)\n",
    "  R[:, 0, 2] = 2 * (x*z + r*y)\n",
    "  R[:, 1, 0] = 2 * (x*y + r*z)\n",
    "  R[:, 1, 1] = 1 - 2 * (x*x + z*z)\n",
    "  R[:, 1, 2] = 2 * (y*z - r*x)\n",
    "  R[:, 2, 0] = 2 * (x*z - r*y)\n",
    "  R[:, 2, 1] = 2 * (y*z + r*x)\n",
    "  R[:, 2, 2] = 1 - 2 * (x*x + y*y)\n",
    "\n",
    "  return R\n",
    "\n",
    "def build_convariance_3d(scale, rot):\n",
    "  S = torch.zeros((scale.shape[0], 3, 3), device=device)\n",
    "  S[:, 0, 0] = scale[:, 0]\n",
    "  S[:, 1, 1] = scale[:, 1]\n",
    "  S[:, 2, 2] = scale[:, 2]\n",
    "  R = build_rotation(rot)\n",
    "  M = torch.bmm(R, S)\n",
    "  Cov3D = torch.bmm(M, M.transpose(1, 2))\n",
    "  return Cov3D\n",
    "\n",
    "\n",
    "def project_cov_2d(cov3d, view_matrix, focal, center, H, W):\n",
    "  # View Matrix (World -> cam)\n",
    "  W_mat = view_matrix[:3, :3]\n",
    "  p_cam = torch.matmul(torch.cat([center, torch.ones_like(center[:, :1])], dim=1), torch.inverse(view_matrix).T)[:, :3]\n",
    "\n",
    "  # Project 3D covariance to Camera Space\n",
    "  T = torch.matmul(torch.matmul(W_mat, cov3d), W_mat.T)\n",
    "\n",
    "  # Simple Perspective projection approximation\n",
    "  z = p_cam[:, 2].clamp(min=0.1)\n",
    "  scale_factor = (focal / z) ** 2\n",
    "\n",
    "  cov2d = torch.zeros((cov3d.shape[0], 2, 2), device=device)\n",
    "  cov2d[:, 0, 0] = T[:, 0, 0] * scale_factor + 0.3\n",
    "  cov2d[:, 0, 1] = T[:, 0, 1] * scale_factor\n",
    "  cov2d[:, 1, 0] = T[:, 1, 0] * scale_factor\n",
    "  cov2d[:, 1, 1] = T[:, 1, 1] * scale_factor + 0.3\n",
    "\n",
    "  return cov2d, p_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6I--9v5ZaJqL"
   },
   "outputs": [],
   "source": [
    "class GaussianModel(nn.Module):\n",
    "  def __init__(self, initial_points=None, initial_colors=None, num_points=2000):\n",
    "    super().__init__()\n",
    "\n",
    "    if initial_points is not None:\n",
    "      print(\"Initializing with real COLMAP points\")\n",
    "      self.means = nn.Parameter(torch.tensor(initial_points).float().to(device))\n",
    "\n",
    "      # Initialize colors from COLMAP (converted to 0-1 range)\n",
    "      # using Inverse Sigmoid so that sigmoid(p) = color\n",
    "      c_norm = torch.tensor(initial_colors).float().to(device) / 255.0\n",
    "      self.colors = nn.Parameter(torch.logit(c_norm.clamp(0.01, 0.99)))\n",
    "\n",
    "      num_points = len(initial_points)\n",
    "\n",
    "    else:\n",
    "      print(\"No points provided, using random initializationnnnnnnnnnnn\")\n",
    "      self.means = nn.Parameter(torch.randn(num_points, 3, device=device) * 4.0 -2.0)\n",
    "      self.colors = nn.Parameter(torch.rand(num_points, 3, device=device))\n",
    "\n",
    "    # Inittialize scales (small log scale)\n",
    "    self.scales = nn.Parameter(torch.full((num_points, 3), -5.0, device=device))\n",
    "    # Rotation (Identity quaternion)\n",
    "    self.quats = nn.Parameter(torch.rand(num_points, 4, device=device))\n",
    "    self.quats.data[:, 0] = 1.0\n",
    "    self.quats.data[:, 1:] = 0.0\n",
    "\n",
    "    # opacity (logit space, start slightly visible)\n",
    "    self.opacities = nn.Parameter(torch.full((num_points, 1), 0.1, device=device))\n",
    "\n",
    "\n",
    "  def forward(self, view_matrix, H, W, focal):\n",
    "    scale = torch.exp(self.scales)\n",
    "    rot = self.quats / self.quats.norm(dim=-1, keepdim=True)\n",
    "    opacity = torch.sigmoid(self.opacities)\n",
    "    color = torch.sigmoid(self.colors)\n",
    "\n",
    "    cov3d = build_convariance_3d(scale, rot)\n",
    "    cov2d, p_cam = project_cov_2d(cov3d, view_matrix, focal, self.means, H, W)\n",
    "\n",
    "    cx, cy = W/2, H/2\n",
    "    screen_x = (p_cam[:, 0] / p_cam[:, 2]) * focal + cx\n",
    "    screen_y = (p_cam[:, 1] / p_cam[:, 2]) * focal + cy\n",
    "    means2d = torch.stack([screen_x, screen_y], dim=1)\n",
    "\n",
    "    return means2d, cov2d, opacity, color, p_cam[:, 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Sp6z891aWVI"
   },
   "outputs": [],
   "source": [
    "def render_gaussian_layer(H, W, means2d, cov2d, opacity, color, depth):\n",
    "    # Create Grid\n",
    "    y, x = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n",
    "    coords = torch.stack([x, y], dim=-1).float()\n",
    "\n",
    "    # Sort (Back to Front for better blending)\n",
    "    sorted_idx = torch.argsort(depth)\n",
    "    means2d = means2d[sorted_idx]\n",
    "    cov2d = cov2d[sorted_idx]\n",
    "    opacity = opacity[sorted_idx]\n",
    "    color = color[sorted_idx]\n",
    "\n",
    "    # Canvas (White Background instead of Black)\n",
    "    canvas = torch.ones((H, W, 3), device=device)\n",
    "\n",
    "    # Filter off-screen points\n",
    "    mask = (means2d[:,0] > 0) & (means2d[:,0] < W) & (means2d[:,1] > 0) & (means2d[:,1] < H)\n",
    "    indices = torch.where(mask)[0]\n",
    "\n",
    "    # RENDER ALL POINTS (No Limit)\n",
    "    # This will be slow! But it will look correct\n",
    "    print(f\"Rendering {len(indices)} splats...\", end='\\r')\n",
    "\n",
    "    for i in indices:\n",
    "        mu = means2d[i]\n",
    "        inv_sigma = torch.inverse(cov2d[i] + torch.eye(2, device=device)*1e-5)\n",
    "\n",
    "        delta = coords - mu\n",
    "        dist = (delta[:,:,0]**2 * inv_sigma[0,0] +\n",
    "                2 * delta[:,:,0]*delta[:,:,1] * inv_sigma[0,1] +\n",
    "                delta[:,:,1]**2 * inv_sigma[1,1])\n",
    "\n",
    "        alpha_val = opacity[i] * torch.exp(-0.5 * dist)\n",
    "        alpha_val = alpha_val[..., None]\n",
    "\n",
    "        # Soft cutoff\n",
    "        alpha_mask = (alpha_val > 0.05).float()\n",
    "        weight = alpha_val * alpha_mask\n",
    "\n",
    "        # Blend\n",
    "        canvas = canvas * (1 - weight) + color[i] * weight\n",
    "\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gauqshwOhScR"
   },
   "outputs": [],
   "source": [
    "def densify_and_prune(model, optimizer, grad_accum, counts, threshold=0.0002):\n",
    "    # Calculate Average Gradient per Point\n",
    "    # We accumulated gradients over N steps, now we average them\n",
    "    avg_grads = grad_accum / counts\n",
    "    avg_grads[avg_grads.isnan()] = 0.0\n",
    "\n",
    "    # Identify Candidates for Densification\n",
    "    # \"High Gradient\" means the point is struggling to fit the image\n",
    "    should_densify = (avg_grads >= threshold)\n",
    "\n",
    "    if not should_densify.any():\n",
    "        return model, optimizer, grad_accum, counts\n",
    "\n",
    "    # Extract current parameters\n",
    "    means = model.means.data\n",
    "    scales = model.scales.data\n",
    "    quats = model.quats.data\n",
    "    colors = model.colors.data\n",
    "    opacities = model.opacities.data\n",
    "\n",
    "    # SPLIT LOGIC (High Grad + Big Scale) ---\n",
    "    # Convert log-scale to actual size to check if it's \"Big\"\n",
    "    # (Threshold 0.05 is an arbitrary size limit for this demo)\n",
    "    big_points = torch.max(torch.exp(scales), dim=1).values > 0.05\n",
    "    split_mask = should_densify & big_points\n",
    "\n",
    "    # CLONE LOGIC (High Grad + Small Scale) ---\n",
    "    clone_mask = should_densify & ~big_points\n",
    "\n",
    "    # 3. Create New Points\n",
    "    new_means, new_scales, new_quats, new_colors, new_opacities = [], [], [], [], []\n",
    "\n",
    "    # Process Clones (Just copy them)\n",
    "    if clone_mask.any():\n",
    "        new_means.append(means[clone_mask])\n",
    "        new_scales.append(scales[clone_mask])\n",
    "        new_quats.append(quats[clone_mask])\n",
    "        new_colors.append(colors[clone_mask])\n",
    "        new_opacities.append(opacities[clone_mask])\n",
    "\n",
    "    # Process Splits (Copy but shrink size)\n",
    "    if split_mask.any():\n",
    "        # Create 2 copies for every split point\n",
    "        # Copy 1\n",
    "        new_means.append(means[split_mask])\n",
    "        # Shrink scale (divide by 1.6 = subtract log(1.6))\n",
    "        new_scales.append(scales[split_mask] - math.log(1.6))\n",
    "        new_quats.append(quats[split_mask])\n",
    "        new_colors.append(colors[split_mask])\n",
    "        new_opacities.append(opacities[split_mask])\n",
    "\n",
    "        # Copy 2 (Move slightly to avoid overlap)\n",
    "        new_means.append(means[split_mask] + (torch.rand_like(means[split_mask])*0.01))\n",
    "        new_scales.append(scales[split_mask] - math.log(1.6))\n",
    "        new_quats.append(quats[split_mask])\n",
    "        new_colors.append(colors[split_mask])\n",
    "        new_opacities.append(opacities[split_mask])\n",
    "\n",
    "    # Concatenate EVERYTHING (Old + New)\n",
    "    if len(new_means) > 0:\n",
    "        total_means = torch.cat([means, *new_means])\n",
    "        total_scales = torch.cat([scales, *new_scales])\n",
    "        total_quats = torch.cat([quats, *new_quats])\n",
    "        total_colors = torch.cat([colors, *new_colors])\n",
    "        total_opacities = torch.cat([opacities, *new_opacities])\n",
    "\n",
    "        # Update Model Parameters\n",
    "        # Note: We must re-wrap them as nn.Parameter\n",
    "        model.means = nn.Parameter(total_means)\n",
    "        model.scales = nn.Parameter(total_scales)\n",
    "        model.quats = nn.Parameter(total_quats)\n",
    "        model.colors = nn.Parameter(total_colors)\n",
    "        model.opacities = nn.Parameter(total_opacities)\n",
    "\n",
    "        # Re-Initialize Optimizer\n",
    "        # (This is the \"Penalty\" of Python - we lose momentum history)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "        # Reset Gradient Accumulators for the new size\n",
    "        grad_accum = torch.zeros((total_means.shape[0], 1), device=device)\n",
    "        counts = torch.zeros((total_means.shape[0], 1), device=device)\n",
    "\n",
    "        print(f\"Densified! {len(means)} -> {len(total_means)} points\")\n",
    "\n",
    "    return model, optimizer, grad_accum, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "365b3b05ba80497a9c2dec3b9629f034",
      "a8f3d7bbca3945cc80bd2a7dd20f056e",
      "de7e0536c4a04105801a76e6e070d738",
      "bcd1da6fe20b4efbaff25ebe367776cb",
      "897bd620c60c4695b3f074ee1e864690",
      "8db5b4bcb02b4c7995313bb27e6be231",
      "bb2e65b779aa483e9ae692e87827a309",
      "0bf62816a38547d381218beed5f929b6",
      "c474b6c3ff3442ec8d0fb3ddd99704be",
      "adfd6e39d62f46e480b8cc87d61157c5",
      "3601f85de3df4522ba9b56847b219916",
      "a2d73a77c49e4cf68be729ffdb4975f8",
      "8a70fd467c7a4ddca8cb41a3cfd86dd9",
      "4ae778510a7b4a06ae5b494667162e73",
      "c86c954520f943e68669dd0773b67531",
      "0ccf0875860a4a2d9ed2bed35ef26db3",
      "e3ef90a888f14a399f22e00c989114e3",
      "6aaf45a6076449d9aa7241b89a2be035",
      "aa3084c0e3bf4b2888c8ae2a21d0009d",
      "b52954b6d57144eba0a017251ce0010b",
      "32a6c7534ff144e6914daa9cecfaff13",
      "ae3357d1ee0c4e3ebbc02ced2f43193c"
     ]
    },
    "id": "DkUKgIzigE1R",
    "outputId": "d86c9e68-f223-41b3-e7c9-649201ed493d"
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "images, poses, focal, H, W = load_data(json_file, dataset_root)\n",
    "init_pts, init_cols = read_points3D_binary(points3d_path)\n",
    "\n",
    "# Downsample\n",
    "if init_pts is not None:\n",
    "    print(f\"Original Cloud: {len(init_pts)} points.\")\n",
    "    target_count = 8000 # Safety limit\n",
    "    if len(init_pts) > target_count:\n",
    "        indices = np.random.choice(len(init_pts), target_count, replace=False)\n",
    "        init_pts = init_pts[indices]\n",
    "        init_cols = init_cols[indices]\n",
    "        print(f\"Downsampled to {len(init_pts)} points for stability.\")\n",
    "\n",
    "# Create Model & Optimizer\n",
    "model = GaussianModel(initial_points=init_pts, initial_colors=init_cols).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# Initialize Gradient Accumulators\n",
    "# These track which points are struggling and need to be split\n",
    "grad_accum = torch.zeros((model.means.shape[0], 1), device=device)\n",
    "counts = torch.zeros((model.means.shape[0], 1), device=device)\n",
    "\n",
    "print(\"===============STARTING GAUSSIAN SPLATTING TRAINING===================\")\n",
    "losses = []\n",
    "\n",
    "for i in tqdm(range(5000)):\n",
    "    img_idx = np.random.randint(len(images))\n",
    "    gt_img = images[img_idx].to(device) # Ensure image is on GPU\n",
    "    pose = poses[img_idx].to(device)\n",
    "\n",
    "    # Forward Pass\n",
    "    means2d, cov2d, opacity, color, depth = model(pose, H, W, focal)\n",
    "\n",
    "    # Keep Gradients for Densification\n",
    "    # PyTorch usually throws away intermediate gradients. We need them to decide who to split.\n",
    "    means2d.retain_grad()\n",
    "\n",
    "    render_img = render_gaussian_layer(H, W, means2d, cov2d, opacity, color, depth)\n",
    "\n",
    "    loss = F.mse_loss(render_img, gt_img)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Accumulate Gradients (The \"Stress\" Tracker)\n",
    "    with torch.no_grad():\n",
    "        if means2d.grad is not None:\n",
    "            # How hard is the gradient pulling this point?\n",
    "            # We must handle potential size mismatches if densification just happened\n",
    "            if means2d.grad.shape[0] == grad_accum.shape[0]:\n",
    "                grad_accum += means2d.grad.norm(dim=1, keepdim=True)\n",
    "                counts += 1\n",
    "\n",
    "    if i > 0 and i % 500 == 0:\n",
    "        model, optimizer, grad_accum, counts = densify_and_prune(\n",
    "            model,\n",
    "            optimizer,\n",
    "            grad_accum,\n",
    "            counts,\n",
    "            threshold=0.0002 # Lower = More points, Higher = Fewer points\n",
    "        )\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        plt.figure(figsize=(6,3))\n",
    "        plt.subplot(1,2,1); plt.imshow(render_img.detach().cpu().numpy()); plt.title(f\"Iter {i} ({len(model.means)} pts)\")\n",
    "        plt.subplot(1,2,2); plt.imshow(gt_img.cpu().numpy()); plt.title(\"Ground Truth\")\n",
    "        plt.show()\n",
    "\n",
    "print(\"TRANININGG COMPLETETEEEEE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9AYmymT3NdM",
    "outputId": "0ff39e46-7a79-45a9-dbfe-e01ad1c53f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /content/drive/MyDrive/dataset/custom-dataset/gaussian_checkpoint.pth...\n",
      "Model saved! You can safely close the notebook now.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/content/drive/MyDrive/dataset/custom-dataset/gaussian_checkpoint.pth\"\n",
    "\n",
    "print(f\"Saving model to {checkpoint_path}...\")\n",
    "\n",
    "torch.save({\n",
    "    'iteration': i,              # Remembers we are at step 500\n",
    "    'model_state': model.state_dict(), # Saves means, colors, opacities, etc.\n",
    "    'optimizer_state': optimizer.state_dict() # Saves the momentum of the training\n",
    "}, checkpoint_path)\n",
    "\n",
    "print(\"Model saved! You can safely close the notebook now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178,
     "referenced_widgets": [
      "9f5a6dd03a7e4e7f9ff254f0056b4584",
      "f54b7f3cd03c4418ba7bd3b8453f1c07",
      "1854463171f24531b457bd07f8899595",
      "6fec9b2d75784debb3da2dfe3b14f942",
      "d02202b8838e4926bca164d02fb3d10f",
      "1edb17956d6944deaedff8430ad00a3b",
      "c9d54b35bc4446f0a2ecedb106249fa7",
      "ab639c29e510449b97c509f32eba4e0d",
      "3c560d237bab438bb2f8110360b476c3",
      "84aa5ed1104b4864a72c06b62d3640e1",
      "db036805f37747ad9bb0116727f12040"
     ]
    },
    "id": "HcH6cb2TEFVk",
    "outputId": "f7a4cfc8-1090-4812-b854-73052612dbbf"
   },
   "outputs": [],
   "source": [
    "\n",
    "images, poses, focal, H, W = load_data(json_file, dataset_root)\n",
    "\n",
    "init_pts, init_cols = read_points3D_binary(points3d_path)\n",
    "\n",
    "# FOR MEMORY CRASH \n",
    "if init_pts is not None:\n",
    "    print(f\"Original Cloud: {len(init_pts)} points. (Too heavy for Python!)\")\n",
    "\n",
    "    # pick 8,000 random points, safe for memory\n",
    "    target_count = 8000\n",
    "\n",
    "    if len(init_pts) > target_count:\n",
    "        indices = np.random.choice(len(init_pts), target_count, replace=False)\n",
    "        init_pts = init_pts[indices]\n",
    "        init_cols = init_cols[indices]\n",
    "        print(f\"Downsampled to {len(init_pts)} points for stability.\")\n",
    "    else:\n",
    "        print(\"Point count is safe.\")\n",
    "\n",
    "\n",
    "model = GaussianModel(initial_points=init_pts, initial_colors=init_cols).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "print(\" Model initialized! NOW you can run the Resume script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "54dc30d9242d4181b221d4c27a92f5ec",
      "63aab2a1fd774db28f568383bf19fe1c",
      "4cc85c33778e4a679aa0bcec4ca72359",
      "969e446df6d04b0885eb2a657fcfa2f8",
      "1b2bcd16c75b44909af266054ad67177",
      "462a6e8fe6d940c785a0791c2596c730",
      "e698e57ba8a143ddaa926738d1ceae11",
      "02bb500a46f1478089d42ed33b0a37c9",
      "8268b7aee6244d6e8f12d2b480dbaf6a",
      "36627ef960304e32b72a4ebed141fdd5",
      "d153a149304c40ef967a5fbca858e34e"
     ]
    },
    "id": "k7CnYTvUTJWB",
    "outputId": "f96b740c-a8ba-4cf5-acc6-6ad3c18bc108"
   },
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_path = \"/content/drive/MyDrive/dataset/custom-dataset/gaussian_checkpoint.pth\"\n",
    "\n",
    "if 'model' not in locals() or 'optimizer' not in locals():\n",
    "    print(\"ERROR: You skipped Step 2! Go run the block you asked about first.\")\n",
    "else:\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Found checkpoint at {checkpoint_path}. Loading...\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "        # Overwrite the fresh model with saved weights\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        start_iter = checkpoint['iteration'] + 1\n",
    "\n",
    "        print(f\"Success! Resuming from iteration {start_iter}\")\n",
    "\n",
    "        \n",
    "        print(\"Continuing Training...\")\n",
    "\n",
    "        \n",
    "        for i in tqdm(range(start_iter, 3001)):\n",
    "            img_idx = np.random.randint(len(images)) # Note: Ensure 'images' variable matches your loader name\n",
    "            gt_img = images[img_idx].to(device) # Send to GPU\n",
    "            pose = poses[img_idx].to(device)\n",
    "\n",
    "            means2d, cov2d, opacity, color, depth = model(pose, H, W, focal)\n",
    "            render_img = render_gaussian_layer(H, W, means2d, cov2d, opacity, color, depth)\n",
    "\n",
    "            loss = F.mse_loss(render_img, gt_img)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "           \n",
    "            if i % 50 == 0:\n",
    "                plt.figure(figsize=(6,3))\n",
    "                plt.subplot(1,2,1); plt.imshow(render_img.detach().cpu().numpy()); plt.title(f\"Iter {i}\")\n",
    "                plt.subplot(1,2,2); plt.imshow(gt_img.cpu().numpy()); plt.title(\"Truth\")\n",
    "                plt.show()\n",
    "\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                torch.save({\n",
    "                    'iteration': i,\n",
    "                    'model_state': model.state_dict(),\n",
    "                    'optimizer_state': optimizer.state_dict()\n",
    "                }, checkpoint_path)\n",
    "                print(\"Checkpoint updated.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}. Did you save it earlier?\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
